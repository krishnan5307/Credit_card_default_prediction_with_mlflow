{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: findspark in c:\\data science\\internship projects\\credit card defaulters\\credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages (2.0.1)\n"]}],"source":["!pip install findspark"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n","| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|default.payment.next.month|\n","+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n","|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|                         1|\n","|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|                         1|\n","|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|                         0|\n","|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|                         0|\n","|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|                         0|\n","|  6|    50000|  1|        1|       2| 37|    0|    0|    0|    0|    0|    0|    64400|    57069|    57608|    19394|    19619|    20024|    2500|    1815|     657|    1000|    1000|     800|                         0|\n","|  7|    5e+05|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0|   367965|   412023|   445007|   542653|   483003|   473944|   55000|   40000|   38000|   20239|   13750|   13770|                         0|\n","|  8|    1e+05|  2|        2|       2| 23|    0|   -1|   -1|    0|    0|   -1|    11876|      380|      601|      221|     -159|      567|     380|     601|       0|     581|    1687|    1542|                         0|\n","|  9|   140000|  2|        3|       1| 28|    0|    0|    2|    0|    0|    0|    11285|    14096|    12108|    12211|    11793|     3719|    3329|       0|     432|    1000|    1000|    1000|                         0|\n","| 10|    20000|  1|        3|       2| 35|   -2|   -2|   -2|   -2|   -1|   -1|        0|        0|        0|        0|    13007|    13912|       0|       0|       0|   13007|    1122|       0|                         0|\n","| 11|    2e+05|  2|        3|       2| 34|    0|    0|    2|    0|    0|   -1|    11073|     9787|     5535|     2513|     1828|     3731|    2306|      12|      50|     300|    3738|      66|                         0|\n","| 12|   260000|  2|        1|       2| 51|   -1|   -1|   -1|   -1|   -1|    2|    12261|    21670|     9966|     8517|    22287|    13668|   21818|    9966|    8583|   22301|       0|    3640|                         0|\n","| 13|   630000|  2|        2|       2| 41|   -1|    0|   -1|   -1|   -1|   -1|    12137|     6500|     6500|     6500|     6500|     2870|    1000|    6500|    6500|    6500|    2870|       0|                         0|\n","| 14|    70000|  1|        2|       2| 30|    1|    2|    2|    0|    0|    2|    65802|    67369|    65701|    66782|    36137|    36894|    3200|       0|    3000|    3000|    1500|       0|                         1|\n","| 15|   250000|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0|    70887|    67060|    63561|    59696|    56875|    55512|    3000|    3000|    3000|    3000|    3000|    3000|                         0|\n","| 16|    50000|  2|        3|       3| 23|    1|    2|    0|    0|    0|    0|    50614|    29173|    28116|    28771|    29531|    30211|       0|    1500|    1100|    1200|    1300|    1100|                         0|\n","| 17|    20000|  1|        1|       2| 24|    0|    0|    2|    2|    2|    2|    15376|    18010|    17428|    18338|    17905|    19104|    3200|       0|    1500|       0|    1650|       0|                         1|\n","| 18|   320000|  1|        1|       1| 49|    0|    0|    0|   -1|   -1|   -1|   253286|   246536|   194663|    70074|     5856|   195599|   10358|   10000|   75940|   20000|  195599|   50000|                         0|\n","| 19|   360000|  2|        1|       1| 49|    1|   -2|   -2|   -2|   -2|   -2|        0|        0|        0|        0|        0|        0|       0|       0|       0|       0|       0|       0|                         0|\n","| 20|   180000|  2|        1|       2| 29|    1|   -2|   -2|   -2|   -2|   -2|        0|        0|        0|        0|        0|        0|       0|       0|       0|       0|       0|       0|                         0|\n","+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_202\"\n","\n","os.environ[\"SPARK_HOME\"] = \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\"\n","os.environ[\"HADOOP_HOME\"]= \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\"\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","df=spark.read.options(delimiter=\",\", header=True).csv(r\"C:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\dataset\\UCI_Credit_Card.csv\")\n","df.show()\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646,"status":"ok","timestamp":1688363813024,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"427ZuB28O-R_","outputId":"5ce7c818-5555-436b-e093-574148515630"},"outputs":[{"data":{"text/plain":["[Row(ID='1', LIMIT_BAL='20000', SEX='2', EDUCATION='2', MARRIAGE='1', AGE='24', PAY_0='2', PAY_2='2', PAY_3='-1', PAY_4='-1', PAY_5='-2', PAY_6='-2', BILL_AMT1='3913', BILL_AMT2='3102', BILL_AMT3='689', BILL_AMT4='0', BILL_AMT5='0', BILL_AMT6='0', PAY_AMT1='0', PAY_AMT2='689', PAY_AMT3='0', PAY_AMT4='0', PAY_AMT5='0', PAY_AMT6='0', default.payment.next.month='1'),\n"," Row(ID='2', LIMIT_BAL='120000', SEX='2', EDUCATION='2', MARRIAGE='2', AGE='26', PAY_0='-1', PAY_2='2', PAY_3='0', PAY_4='0', PAY_5='0', PAY_6='2', BILL_AMT1='2682', BILL_AMT2='1725', BILL_AMT3='2682', BILL_AMT4='3272', BILL_AMT5='3455', BILL_AMT6='3261', PAY_AMT1='0', PAY_AMT2='1000', PAY_AMT3='1000', PAY_AMT4='1000', PAY_AMT5='0', PAY_AMT6='2000', default.payment.next.month='1'),\n"," Row(ID='3', LIMIT_BAL='90000', SEX='2', EDUCATION='2', MARRIAGE='2', AGE='34', PAY_0='0', PAY_2='0', PAY_3='0', PAY_4='0', PAY_5='0', PAY_6='0', BILL_AMT1='29239', BILL_AMT2='14027', BILL_AMT3='13559', BILL_AMT4='14331', BILL_AMT5='14948', BILL_AMT6='15549', PAY_AMT1='1518', PAY_AMT2='1500', PAY_AMT3='1000', PAY_AMT4='1000', PAY_AMT5='1000', PAY_AMT6='5000', default.payment.next.month='0'),\n"," Row(ID='4', LIMIT_BAL='50000', SEX='2', EDUCATION='2', MARRIAGE='1', AGE='37', PAY_0='0', PAY_2='0', PAY_3='0', PAY_4='0', PAY_5='0', PAY_6='0', BILL_AMT1='46990', BILL_AMT2='48233', BILL_AMT3='49291', BILL_AMT4='28314', BILL_AMT5='28959', BILL_AMT6='29547', PAY_AMT1='2000', PAY_AMT2='2019', PAY_AMT3='1200', PAY_AMT4='1100', PAY_AMT5='1069', PAY_AMT6='1000', default.payment.next.month='0'),\n"," Row(ID='5', LIMIT_BAL='50000', SEX='1', EDUCATION='2', MARRIAGE='1', AGE='57', PAY_0='-1', PAY_2='0', PAY_3='-1', PAY_4='0', PAY_5='0', PAY_6='0', BILL_AMT1='8617', BILL_AMT2='5670', BILL_AMT3='35835', BILL_AMT4='20940', BILL_AMT5='19146', BILL_AMT6='19131', PAY_AMT1='2000', PAY_AMT2='36681', PAY_AMT3='10000', PAY_AMT4='9000', PAY_AMT5='689', PAY_AMT6='679', default.payment.next.month='0')]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.head(5)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1688363813024,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"XfnUsCSbPByw","outputId":"6db2570d-a15c-4d7e-ead4-cf7775bc3bf3"},"outputs":[{"data":{"text/plain":["25"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(df.columns)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1412,"status":"ok","timestamp":1688363814433,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"LjiaWeIhPQL3","outputId":"c57ff668-ca23-45d9-a8c5-3523528a7872"},"outputs":[{"data":{"text/plain":["30000"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.count()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1688363814434,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"TlbRVM1APrjL","outputId":"48841a6f-200e-46c2-f522-9a608623aaa1"},"outputs":[{"data":{"text/plain":["<bound method DataFrame.printSchema of DataFrame[ID: string, LIMIT_BAL: string, SEX: string, EDUCATION: string, MARRIAGE: string, AGE: string, PAY_0: string, PAY_2: string, PAY_3: string, PAY_4: string, PAY_5: string, PAY_6: string, BILL_AMT1: string, BILL_AMT2: string, BILL_AMT3: string, BILL_AMT4: string, BILL_AMT5: string, BILL_AMT6: string, PAY_AMT1: string, PAY_AMT2: string, PAY_AMT3: string, PAY_AMT4: string, PAY_AMT5: string, PAY_AMT6: string, default.payment.next.month: string]>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df.printSchema"]},{"cell_type":"markdown","metadata":{"id":"NsJJmR7wRRzQ"},"source":["since all is string and we need to typecast, so we check with pandas for the datatype"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"prHDL3afQleT"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NYhqVeLhQ0Ba"},"outputs":[],"source":["df =pd.read_csv(r\"C:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\dataset\\credit_card.csv\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1688363820188,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"oAv6McarQ4lW","outputId":"9af6ed23-f264-454a-a628-35be8b195f96"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>LIMIT_BAL</th>\n","      <th>SEX</th>\n","      <th>EDUCATION</th>\n","      <th>MARRIAGE</th>\n","      <th>AGE</th>\n","      <th>PAY_0</th>\n","      <th>PAY_2</th>\n","      <th>PAY_3</th>\n","      <th>PAY_4</th>\n","      <th>...</th>\n","      <th>BILL_AMT4</th>\n","      <th>BILL_AMT5</th>\n","      <th>BILL_AMT6</th>\n","      <th>PAY_AMT1</th>\n","      <th>PAY_AMT2</th>\n","      <th>PAY_AMT3</th>\n","      <th>PAY_AMT4</th>\n","      <th>PAY_AMT5</th>\n","      <th>PAY_AMT6</th>\n","      <th>default.payment.next.month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>20000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>689.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>120000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>-1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3272.0</td>\n","      <td>3455.0</td>\n","      <td>3261.0</td>\n","      <td>0.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>0.0</td>\n","      <td>2000.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>90000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>14331.0</td>\n","      <td>14948.0</td>\n","      <td>15549.0</td>\n","      <td>1518.0</td>\n","      <td>1500.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>1000.0</td>\n","      <td>5000.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>50000.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>28314.0</td>\n","      <td>28959.0</td>\n","      <td>29547.0</td>\n","      <td>2000.0</td>\n","      <td>2019.0</td>\n","      <td>1200.0</td>\n","      <td>1100.0</td>\n","      <td>1069.0</td>\n","      <td>1000.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>50000.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>57</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>-1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>20940.0</td>\n","      <td>19146.0</td>\n","      <td>19131.0</td>\n","      <td>2000.0</td>\n","      <td>36681.0</td>\n","      <td>10000.0</td>\n","      <td>9000.0</td>\n","      <td>689.0</td>\n","      <td>679.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 25 columns</p>\n","</div>"],"text/plain":["   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n","0   1    20000.0    2          2         1   24      2      2     -1     -1   \n","1   2   120000.0    2          2         2   26     -1      2      0      0   \n","2   3    90000.0    2          2         2   34      0      0      0      0   \n","3   4    50000.0    2          2         1   37      0      0      0      0   \n","4   5    50000.0    1          2         1   57     -1      0     -1      0   \n","\n","   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n","0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n","1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n","2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n","3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n","4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n","\n","   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n","0       0.0       0.0       0.0                           1  \n","1    1000.0       0.0    2000.0                           1  \n","2    1000.0    1000.0    5000.0                           0  \n","3    1100.0    1069.0    1000.0                           0  \n","4    9000.0     689.0     679.0                           0  \n","\n","[5 rows x 25 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df.head(5)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1688363820593,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"OarOL80mREBg","outputId":"89aa791c-16b1-481b-9459-611cddc13e76"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 30000 entries, 0 to 29999\n","Data columns (total 25 columns):\n"," #   Column                      Non-Null Count  Dtype  \n","---  ------                      --------------  -----  \n"," 0   ID                          30000 non-null  int64  \n"," 1   LIMIT_BAL                   30000 non-null  float64\n"," 2   SEX                         30000 non-null  int64  \n"," 3   EDUCATION                   30000 non-null  int64  \n"," 4   MARRIAGE                    30000 non-null  int64  \n"," 5   AGE                         30000 non-null  int64  \n"," 6   PAY_0                       30000 non-null  int64  \n"," 7   PAY_2                       30000 non-null  int64  \n"," 8   PAY_3                       30000 non-null  int64  \n"," 9   PAY_4                       30000 non-null  int64  \n"," 10  PAY_5                       30000 non-null  int64  \n"," 11  PAY_6                       30000 non-null  int64  \n"," 12  BILL_AMT1                   30000 non-null  float64\n"," 13  BILL_AMT2                   30000 non-null  float64\n"," 14  BILL_AMT3                   30000 non-null  float64\n"," 15  BILL_AMT4                   30000 non-null  float64\n"," 16  BILL_AMT5                   30000 non-null  float64\n"," 17  BILL_AMT6                   30000 non-null  float64\n"," 18  PAY_AMT1                    30000 non-null  float64\n"," 19  PAY_AMT2                    30000 non-null  float64\n"," 20  PAY_AMT3                    30000 non-null  float64\n"," 21  PAY_AMT4                    30000 non-null  float64\n"," 22  PAY_AMT5                    30000 non-null  float64\n"," 23  PAY_AMT6                    30000 non-null  float64\n"," 24  default.payment.next.month  30000 non-null  int64  \n","dtypes: float64(13), int64(12)\n","memory usage: 5.7 MB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Jc7a4QMWWLEt"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n","from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import StandardScaler\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.sql.types import StringType, IntegerType\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType\n","from pyspark.sql.functions import col, count, isnan, when\n","from pyspark.mllib.evaluation import BinaryClassificationMetrics\n","from pyspark.mllib.evaluation import MulticlassMetrics\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"8QMBn06oRI4q"},"outputs":[],"source":["# Define the schema (typecasting\n","schema = StructType([\n","\n","StructField(\"ID\"        ,  IntegerType(), nullable=False),\n","StructField(\"LIMIT_BAL\" ,  FloatType(),   nullable=False),\n","StructField(\"SEX\"       ,  IntegerType(), nullable=False),\n","StructField(\"EDUCATION\" ,  IntegerType(), nullable=False),\n","StructField(\"MARRIAGE\"  ,  IntegerType(), nullable=False),\n","StructField(\"AGE\"       ,  IntegerType(), nullable=False),\n","StructField(\"PAY_0\"     ,  IntegerType(), nullable=False),\n","StructField(\"PAY_2\"     ,  IntegerType(), nullable=False),\n","StructField(\"PAY_3\"     ,  IntegerType(), nullable=False),\n","StructField(\"PAY_4\"     ,  IntegerType(), nullable=False),\n","StructField(\"PAY_5\"     ,  IntegerType(), nullable=False),\n","StructField(\"PAY_6\"     ,  IntegerType(), nullable=False),\n","StructField(\"BILL_AMT1\" ,  FloatType(),   nullable=False),\n","StructField(\"BILL_AMT2\" ,  FloatType(),   nullable=False),\n","StructField(\"BILL_AMT3\" ,  FloatType(),   nullable=False),\n","StructField(\"BILL_AMT4\" ,  FloatType(),   nullable=False),\n","StructField(\"BILL_AMT5\" ,  FloatType(),   nullable=False),\n","StructField(\"BILL_AMT6\" ,  FloatType(),   nullable=False),\n","StructField(\"PAY_AMT1\"  ,  FloatType(),   nullable=False),\n","StructField(\"PAY_AMT2\"  ,  FloatType(),   nullable=False),\n","StructField(\"PAY_AMT3\"  ,  FloatType(),   nullable=False),\n","StructField(\"PAY_AMT4\"  ,  FloatType(),   nullable=False),\n","StructField(\"PAY_AMT5\"  ,  FloatType(),   nullable=False),\n","StructField(\"PAY_AMT6\"  ,  FloatType(),   nullable=False),\n","StructField(\"default.payment.next.month\", IntegerType(), nullable=False)\n","\n","])\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"sihsCK8QWTMB"},"outputs":[],"source":["df = spark.read.csv(r\"C:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\dataset\\UCI_Credit_Card.csv\",header=True, schema= schema)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1688363825602,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"tNTeH2wJW4V5","outputId":"6a61a8b8-3ebd-4935-ddaa-800f29cb6c6f"},"outputs":[{"data":{"text/plain":["<bound method DataFrame.printSchema of DataFrame[ID: int, LIMIT_BAL: float, SEX: int, EDUCATION: int, MARRIAGE: int, AGE: int, PAY_0: int, PAY_2: int, PAY_3: int, PAY_4: int, PAY_5: int, PAY_6: int, BILL_AMT1: float, BILL_AMT2: float, BILL_AMT3: float, BILL_AMT4: float, BILL_AMT5: float, BILL_AMT6: float, PAY_AMT1: float, PAY_AMT2: float, PAY_AMT3: float, PAY_AMT4: float, PAY_AMT5: float, PAY_AMT6: float, default.payment.next.month: int]>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df.printSchema"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":822,"status":"ok","timestamp":1688363827824,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"-7A7MKYnW7Q0","outputId":"c7e87c45-a970-4804-d6c5-2e18d49abab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n","| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|default.payment.next.month|\n","+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n","|  1|  20000.0|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|   3913.0|   3102.0|    689.0|      0.0|      0.0|      0.0|     0.0|   689.0|     0.0|     0.0|     0.0|     0.0|                         1|\n","|  2| 120000.0|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|   2682.0|   1725.0|   2682.0|   3272.0|   3455.0|   3261.0|     0.0|  1000.0|  1000.0|  1000.0|     0.0|  2000.0|                         1|\n","|  3|  90000.0|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|  29239.0|  14027.0|  13559.0|  14331.0|  14948.0|  15549.0|  1518.0|  1500.0|  1000.0|  1000.0|  1000.0|  5000.0|                         0|\n","|  4|  50000.0|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|  46990.0|  48233.0|  49291.0|  28314.0|  28959.0|  29547.0|  2000.0|  2019.0|  1200.0|  1100.0|  1069.0|  1000.0|                         0|\n","|  5|  50000.0|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|   8617.0|   5670.0|  35835.0|  20940.0|  19146.0|  19131.0|  2000.0| 36681.0| 10000.0|  9000.0|   689.0|   679.0|                         0|\n","|  6|  50000.0|  1|        1|       2| 37|    0|    0|    0|    0|    0|    0|  64400.0|  57069.0|  57608.0|  19394.0|  19619.0|  20024.0|  2500.0|  1815.0|   657.0|  1000.0|  1000.0|   800.0|                         0|\n","|  7| 500000.0|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0| 367965.0| 412023.0| 445007.0| 542653.0| 483003.0| 473944.0| 55000.0| 40000.0| 38000.0| 20239.0| 13750.0| 13770.0|                         0|\n","|  8| 100000.0|  2|        2|       2| 23|    0|   -1|   -1|    0|    0|   -1|  11876.0|    380.0|    601.0|    221.0|   -159.0|    567.0|   380.0|   601.0|     0.0|   581.0|  1687.0|  1542.0|                         0|\n","|  9| 140000.0|  2|        3|       1| 28|    0|    0|    2|    0|    0|    0|  11285.0|  14096.0|  12108.0|  12211.0|  11793.0|   3719.0|  3329.0|     0.0|   432.0|  1000.0|  1000.0|  1000.0|                         0|\n","| 10|  20000.0|  1|        3|       2| 35|   -2|   -2|   -2|   -2|   -1|   -1|      0.0|      0.0|      0.0|      0.0|  13007.0|  13912.0|     0.0|     0.0|     0.0| 13007.0|  1122.0|     0.0|                         0|\n","| 11| 200000.0|  2|        3|       2| 34|    0|    0|    2|    0|    0|   -1|  11073.0|   9787.0|   5535.0|   2513.0|   1828.0|   3731.0|  2306.0|    12.0|    50.0|   300.0|  3738.0|    66.0|                         0|\n","| 12| 260000.0|  2|        1|       2| 51|   -1|   -1|   -1|   -1|   -1|    2|  12261.0|  21670.0|   9966.0|   8517.0|  22287.0|  13668.0| 21818.0|  9966.0|  8583.0| 22301.0|     0.0|  3640.0|                         0|\n","| 13| 630000.0|  2|        2|       2| 41|   -1|    0|   -1|   -1|   -1|   -1|  12137.0|   6500.0|   6500.0|   6500.0|   6500.0|   2870.0|  1000.0|  6500.0|  6500.0|  6500.0|  2870.0|     0.0|                         0|\n","| 14|  70000.0|  1|        2|       2| 30|    1|    2|    2|    0|    0|    2|  65802.0|  67369.0|  65701.0|  66782.0|  36137.0|  36894.0|  3200.0|     0.0|  3000.0|  3000.0|  1500.0|     0.0|                         1|\n","| 15| 250000.0|  1|        1|       2| 29|    0|    0|    0|    0|    0|    0|  70887.0|  67060.0|  63561.0|  59696.0|  56875.0|  55512.0|  3000.0|  3000.0|  3000.0|  3000.0|  3000.0|  3000.0|                         0|\n","| 16|  50000.0|  2|        3|       3| 23|    1|    2|    0|    0|    0|    0|  50614.0|  29173.0|  28116.0|  28771.0|  29531.0|  30211.0|     0.0|  1500.0|  1100.0|  1200.0|  1300.0|  1100.0|                         0|\n","| 17|  20000.0|  1|        1|       2| 24|    0|    0|    2|    2|    2|    2|  15376.0|  18010.0|  17428.0|  18338.0|  17905.0|  19104.0|  3200.0|     0.0|  1500.0|     0.0|  1650.0|     0.0|                         1|\n","| 18| 320000.0|  1|        1|       1| 49|    0|    0|    0|   -1|   -1|   -1| 253286.0| 246536.0| 194663.0|  70074.0|   5856.0| 195599.0| 10358.0| 10000.0| 75940.0| 20000.0|195599.0| 50000.0|                         0|\n","| 19| 360000.0|  2|        1|       1| 49|    1|   -2|   -2|   -2|   -2|   -2|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|                         0|\n","| 20| 180000.0|  2|        1|       2| 29|    1|   -2|   -2|   -2|   -2|   -2|      0.0|      0.0|      0.0|      0.0|      0.0|      0.0|     0.0|     0.0|     0.0|     0.0|     0.0|     0.0|                         0|\n","+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["df.show()"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"tHFII2SZa61M"},"outputs":[],"source":["df = df.withColumnRenamed(\"default.payment.next.month\", \"default_payment_next_month\")"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"jx5_uH8tXQen"},"outputs":[],"source":["## no need of strngindexer as there are no catergorcal varivale"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1688363831357,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"ctEpLkX0YjlI","outputId":"2d9354f7-4e73-4a00-f8e9-48a83ab3aa5c"},"outputs":[{"data":{"text/plain":["['ID',\n"," 'LIMIT_BAL',\n"," 'SEX',\n"," 'EDUCATION',\n"," 'MARRIAGE',\n"," 'AGE',\n"," 'PAY_0',\n"," 'PAY_2',\n"," 'PAY_3',\n"," 'PAY_4',\n"," 'PAY_5',\n"," 'PAY_6',\n"," 'BILL_AMT1',\n"," 'BILL_AMT2',\n"," 'BILL_AMT3',\n"," 'BILL_AMT4',\n"," 'BILL_AMT5',\n"," 'BILL_AMT6',\n"," 'PAY_AMT1',\n"," 'PAY_AMT2',\n"," 'PAY_AMT3',\n"," 'PAY_AMT4',\n"," 'PAY_AMT5',\n"," 'PAY_AMT6',\n"," 'default_payment_next_month']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["cols = df.columns\n","cols"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"0jTRtti2Yp3u"},"outputs":[],"source":["cols.remove(\"default_payment_next_month\")"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1688363834089,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"lcpmlOr_aVor","outputId":"78951bd7-7d69-4ecb-f7c5-40730f9ff308"},"outputs":[{"data":{"text/plain":["[Row(ID=1, LIMIT_BAL=20000.0, SEX=2, EDUCATION=2, MARRIAGE=1, AGE=24, PAY_0=2, PAY_2=2, PAY_3=-1, PAY_4=-1, PAY_5=-2, PAY_6=-2, BILL_AMT1=3913.0, BILL_AMT2=3102.0, BILL_AMT3=689.0, BILL_AMT4=0.0, BILL_AMT5=0.0, BILL_AMT6=0.0, PAY_AMT1=0.0, PAY_AMT2=689.0, PAY_AMT3=0.0, PAY_AMT4=0.0, PAY_AMT5=0.0, PAY_AMT6=0.0, default_payment_next_month=1)]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df.head(1)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"qwOrPe4dYvWQ"},"outputs":[],"source":["# Create VectorAssembler\n","##creating new vector /array column for all combined independent features\n","## in vector assembler we are providing input cols and output cols during defining\n","\n","assembler = VectorAssembler(inputCols= cols, outputCol= \"vectorized_features\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"qZbXuZQkZcYw"},"outputs":[],"source":["df_vector= assembler.transform(df)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1668,"status":"ok","timestamp":1688363838518,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"F6Ntmk6oZpb_","outputId":"203617dd-1a76-446e-fcc8-e648ba13ed59"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+\n","|default_payment_next_month| vectorized_features|\n","+--------------------------+--------------------+\n","|                         1|[1.0,20000.0,2.0,...|\n","|                         1|[2.0,120000.0,2.0...|\n","|                         0|[3.0,90000.0,2.0,...|\n","|                         0|[4.0,50000.0,2.0,...|\n","|                         0|[5.0,50000.0,1.0,...|\n","|                         0|[6.0,50000.0,1.0,...|\n","|                         0|[7.0,500000.0,1.0...|\n","|                         0|[8.0,100000.0,2.0...|\n","|                         0|[9.0,140000.0,2.0...|\n","|                         0|[10.0,20000.0,1.0...|\n","|                         0|[11.0,200000.0,2....|\n","|                         0|[12.0,260000.0,2....|\n","|                         0|[13.0,630000.0,2....|\n","|                         1|[14.0,70000.0,1.0...|\n","|                         0|[15.0,250000.0,1....|\n","|                         0|[16.0,50000.0,2.0...|\n","|                         1|[17.0,20000.0,1.0...|\n","|                         0|[18.0,320000.0,1....|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","+--------------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["df_vector=df_vector.select(\"default_payment_next_month\",\"vectorized_features\")\n","df_vector.show()"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3293,"status":"ok","timestamp":1688363843294,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"LYf_MUqsbJDr","outputId":"8962db8e-7955-4d02-d337-ee8f931ce463"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+\n","|default_payment_next_month|      scaledFeatures|\n","+--------------------------+--------------------+\n","|                         1|[1.15468129385139...|\n","|                         1|[2.30936258770278...|\n","|                         0|[3.46404388155417...|\n","|                         0|[4.61872517540556...|\n","|                         0|[5.77340646925695...|\n","|                         0|[6.92808776310834...|\n","|                         0|[8.08276905695973...|\n","|                         0|[9.23745035081112...|\n","|                         0|[0.00103921316446...|\n","|                         0|[0.00115468129385...|\n","|                         0|[0.00127014942323...|\n","|                         0|[0.00138561755262...|\n","|                         0|[0.00150108568200...|\n","|                         1|[0.00161655381139...|\n","|                         0|[0.00173202194077...|\n","|                         0|[0.00184749007016...|\n","|                         1|[0.00196295819954...|\n","|                         0|[0.00207842632893...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","+--------------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["## feature scaling using standard scaler\n","## ## in StandardScaler we are providing input cols and output cols during defining\n","\n","scaler = StandardScaler(inputCol= \"vectorized_features\", outputCol= \"scaledFeatures\")\n","scaled_df= scaler.fit(df_vector).transform(df_vector)\n","scaled_df=scaled_df.select(\"default_payment_next_month\", \"scaledFeatures\")\n","scaled_df.show()"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"ttijpewcjccE"},"outputs":[],"source":["from pyspark.sql.functions import rand\n","\n","# Perform stratified shuffling\n","stratified_df = scaled_df.orderBy(rand())\n","\n","# Split the data into train and test sets\n","train_df, test_df = stratified_df.randomSplit([0.7, 0.3], seed=42)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6651,"status":"ok","timestamp":1688363854158,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"v1N1a9-2jnGj","outputId":"6116cfd3-e8e3-4f88-ae5c-e99c66fe0b14"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+\n","|default_payment_next_month|      scaledFeatures|\n","+--------------------------+--------------------+\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","|                         0|(24,[0,1,2,3,4,5,...|\n","+--------------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["test_df.show()"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2504,"status":"ok","timestamp":1688364863103,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"ZKQUDfpdW5jv","outputId":"728d0788-b4f9-4259-d463-91f8e6b5726c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 0.0920281   1.77267164  4.08889924  1.26526437  3.83164077  3.1460514\n"," -1.77967368 -1.67058422 -1.67102865 -1.71066113 -1.76493313 -1.73914915\n","  0.          0.          0.          0.          0.          0.\n","  0.          0.          0.          0.          0.          0.        ]\n"]}],"source":["row_0 = test_df.select(\"scaledFeatures\").collect()[0]   ## this will give output in SparseVector\n","# Row(scaledFeatures=SparseVector(24, {0: 0.0732, 1: 1.6185, 2: 4.0889, 3: 2.5305, 4: 1.9158, 5: 5.0988, 6: -1.7797, 7: -1.6706, 8: -1.671, 9: -1.7107, 10: -1.7649, 11: -1.7391}))\n","# converting to array\n","scaled_features = row_0.scaledFeatures.toArray()\n","print(scaled_features)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1688365659192,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"Bod8eYH-biOn","outputId":"7ec7608d-bdbb-4d17-df72-d74e0828631b"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["scaled_features.ndim"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":13981,"status":"ok","timestamp":1688368442980,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"S-v1mZPSb-sy"},"outputs":[],"source":["## building ML randomforest algorithm\n","\n","mod= RandomForestClassifier(labelCol=\"default_payment_next_month\", featuresCol= \"scaledFeatures\")\n","model= mod.fit(train_df)"]},{"cell_type":"markdown","metadata":{"id":"QiZzh2U2pCsu"},"source":["## Note: in spark we only have fit() for trainig model, and we use transfrom() for test data to make prediction  but in case of preprocesing feature scling like standarscaler() we use both fit and transform"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"d3VLBfCcnzlE"},"outputs":[],"source":["prediction_df = model.transform(test_df)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2387,"status":"ok","timestamp":1688363884660,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"p4x3sWlYpQjx","outputId":"e5684acb-aa0d-4a6c-c72b-2af0b0b3fcd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+--------------------+--------------------+----------+\n","|default_payment_next_month|      scaledFeatures|       rawPrediction|         probability|prediction|\n","+--------------------------+--------------------+--------------------+--------------------+----------+\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0789458082790...|[0.80394729041395...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0789458082790...|[0.80394729041395...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.2146980879143...|[0.81073490439571...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0789458082790...|[0.80394729041395...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1773208979075...|[0.80886604489537...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0789458082790...|[0.80394729041395...|       0.0|\n","+--------------------------+--------------------+--------------------+--------------------+----------+\n","only showing top 20 rows\n","\n"]}],"source":["prediction_df.show()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["type(prediction_df)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2616,"status":"ok","timestamp":1688222082034,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"Y7CcFKtO2wAq","outputId":"c7683835-5ef2-48db-8b57-9f69229ba9e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy 0.6610960062326668\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["## Binary class evalution\n","\n","eval_binary = BinaryClassificationEvaluator(labelCol=\"default_payment_next_month\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n","## here we can see y_test = \"default_payment_next_month\" and y_pred = \"prediction\" based on predcition dataframe\n","accuracy_binary = eval_binary.evaluate(prediction_df)  ## passing the predicted datafame\n","print(\"accuracy\", accuracy_binary)"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["# Convert 'prediction', 'probability', and 'label' columns to RDD\n","predictionAndLabels = prediction_df.select(\"prediction\", \"probability\", \"default_payment_next_month\")\n","type(predictionAndLabels)"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+--------------------+--------------------------+\n","|prediction|         probability|default_payment_next_month|\n","+----------+--------------------+--------------------------+\n","|       0.0|[0.80759508578860...|                         0|\n","|       0.0|[0.80759508578860...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.80759508578860...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.76923956502745...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.77761241115028...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.80759508578860...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.78792157424118...|                         0|\n","|       0.0|[0.80759508578860...|                         0|\n","+----------+--------------------+--------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["predictionAndLabels.show()"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.sql.column.Column"]},"execution_count":173,"metadata":{},"output_type":"execute_result"}],"source":["type(predictionAndLabels['default_payment_next_month'])\n","type(predictionAndLabels['prediction'])\n"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}],"source":["# Convert the DataFrame to an RDD of (prediction, label) tuples\n","predictionAndLabels = predictionAndLabels.rdd.map(lambda row: (float(row['prediction']), float(row['default_payment_next_month'])))\n","# Create a BinaryClassificationMetrics object\n","metrics = BinaryClassificationMetrics(predictionAndLabels)"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":176,"metadata":{},"output_type":"execute_result"}],"source":["type(predictionAndLabels)"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["# # Create a BinaryClassificationMetrics object\n","# metrics = BinaryClassificationMetrics(predictionAndLabels.map(lambda x: (x[0], x[1])))\n","\n","# # Calculate the area under the ROC curve (AUC-ROC)\n","# auc_roc = metrics.areaUnderROC\n","# print(\"AUC-ROC: \", auc_roc)\n"]},{"cell_type":"code","execution_count":178,"metadata":{"id":"nZsM8ibg7WAd"},"outputs":[],"source":["from pyspark.sql.functions import udf\n","from pyspark.ml.linalg import DenseVector\n","\n","# Assuming 'df' is your DataFrame with the 'probability' column\n","\n","# Define a UDF to extract the probability value from the dense vector\n","extract_probability = udf(lambda x: float(DenseVector(x).values[1]), DoubleType())\n","\n","# Apply the UDF to the 'probability' column and create a new column 'probability_double'\n","prediction_df = prediction_df.withColumn('probability_double', extract_probability('probability'))\n"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":179,"metadata":{},"output_type":"execute_result"}],"source":["type(prediction_df)"]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["# from pyspark.mllib.evaluation import BinaryClassificationMetrics\n","\n","# # Create an example DataFrame with predictions and labels\n","# predictions_and_labels = [(0.1, 0), (0.7, 1), (0.4, 1), (0.9, 1), (0.2, 0)]\n","# df = spark.createDataFrame(predictions_and_labels, [\"prediction\", \"label\"])\n","\n","# # Create a BinaryClassificationMetrics object\n","# metrics = BinaryClassificationMetrics(df.rdd.map(lambda x: (x[0], x[1])))\n","\n","# # Calculate the area under the ROC curve (AUC-ROC)\n","# auc_roc = metrics.areaUnderROC\n","# print(\"AUC-ROC: \", auc_roc)\n","\n","# # You can also calculate other metrics, such as precision, recall, and F1-score using BinaryClassificationMetrics.\n"]},{"cell_type":"code","execution_count":181,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":569,"status":"ok","timestamp":1688222993624,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"p3ZRZO3J5ZtS","outputId":"b28485f7-0bca-4330-b16b-254b64f95753"},"outputs":[{"data":{"text/plain":["<bound method DataFrame.printSchema of DataFrame[default_payment_next_month: int, scaledFeatures: vector, rawPrediction: vector, probability: vector, prediction: double, probability_double: double]>"]},"execution_count":181,"metadata":{},"output_type":"execute_result"}],"source":["prediction_df.printSchema"]},{"cell_type":"code","execution_count":182,"metadata":{"id":"O8kFhcn-4ERr"},"outputs":[],"source":["# Convert 'prediction', 'probability', and 'label' columns to RDD\n","predictionAndLabels = prediction_df.select(\"prediction\", \"probability_double\", \"default_payment_next_month\").rdd\n","\n","# Instantiate BinaryClassificationMetrics\n","binaryMetrics = BinaryClassificationMetrics(predictionAndLabels)\n"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"data":{"text/plain":["<pyspark.mllib.evaluation.BinaryClassificationMetrics at 0x26ef72590d0>"]},"execution_count":183,"metadata":{},"output_type":"execute_result"}],"source":[" binaryMetrics\n"]},{"cell_type":"code","execution_count":184,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2799,"status":"ok","timestamp":1688223489852,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"jlxSDLCC4i36","outputId":"f809c657-67b8-497e-ef94-97e04949afd8"},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function JavaModelWrapper.__del__ at 0x0000026EDF66AAF0>\n","Traceback (most recent call last):\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py\", line 152, in __del__\n","    assert self._sc._gateway is not None\n","AttributeError: 'BinaryClassificationMetrics' object has no attribute '_sc'\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1842.areaUnderROC.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 377.0 failed 1 times, most recent failure: Lost task 1.0 in stage 377.0 (TID 362) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:192)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:181)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:275)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:106)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:126)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[184], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Calculate AUC\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m auc \u001b[39m=\u001b[39m binaryMetrics\u001b[39m.\u001b[39;49mareaUnderROC\n\u001b[0;32m      4\u001b[0m \u001b[39m# Print AUC\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mArea Under ROC Curve (AUC):\u001b[39m\u001b[39m\"\u001b[39m, auc)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:95\u001b[0m, in \u001b[0;36mBinaryClassificationMetrics.areaUnderROC\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m     89\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m1.4.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mareaUnderROC\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m    Computes the area under the receiver operating characteristic\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m    (ROC) curve.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m\"\u001b[39;49m\u001b[39mareaUnderROC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39ma: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m callJavaFunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sc, \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_model, name), \u001b[39m*\u001b[39;49ma)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1842.areaUnderROC.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 377.0 failed 1 times, most recent failure: Lost task 1.0 in stage 377.0 (TID 362) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:192)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:181)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:275)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:106)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:126)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"]}],"source":["# Calculate AUC\n","auc = binaryMetrics.areaUnderROC\n","\n","# Print AUC\n","print(\"Area Under ROC Curve (AUC):\", auc)"]},{"cell_type":"code","execution_count":185,"metadata":{"id":"9ycBAIxh8ril"},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1842.areaUnderPR.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 380.0 failed 1 times, most recent failure: Lost task 2.0 in stage 380.0 (TID 366) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:192)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:181)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:275)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.pr(BinaryClassificationMetrics.scala:137)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderPR(BinaryClassificationMetrics.scala:152)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[185], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Calculate confusion matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m areaUnderPR \u001b[39m=\u001b[39m binaryMetrics\u001b[39m.\u001b[39;49mareaUnderPR\n\u001b[0;32m      3\u001b[0m areaUnderPR\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:103\u001b[0m, in \u001b[0;36mBinaryClassificationMetrics.areaUnderPR\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m     98\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m1.4.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mareaUnderPR\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    Computes the area under the precision-recall curve.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m\"\u001b[39;49m\u001b[39mareaUnderPR\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39ma: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m callJavaFunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sc, \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_model, name), \u001b[39m*\u001b[39;49ma)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1842.areaUnderPR.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 380.0 failed 1 times, most recent failure: Lost task 2.0 in stage 380.0 (TID 366) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:192)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:181)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:275)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.pr(BinaryClassificationMetrics.scala:137)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderPR(BinaryClassificationMetrics.scala:152)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"]}],"source":["# Calculate confusion matrix\n","areaUnderPR = binaryMetrics.areaUnderPR\n","areaUnderPR"]},{"cell_type":"code","execution_count":152,"metadata":{"id":"LrJP6jHjxaPL"},"outputs":[{"ename":"AttributeError","evalue":"'builtin_function_or_method' object has no attribute 'evaluate'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[152], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m class_label \u001b[39min\u001b[39;00m classes:\n\u001b[0;32m      9\u001b[0m     class_predictions \u001b[39m=\u001b[39m predictionAndLabels\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: (\u001b[39m1.0\u001b[39m \u001b[39mif\u001b[39;00m x[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m class_label \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m, x[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 10\u001b[0m     roc_auc_scores[class_label] \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(class_predictions)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Print the results\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m class_label, score \u001b[39min\u001b[39;00m roc_auc_scores\u001b[39m.\u001b[39mitems():\n","\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'evaluate'"]}],"source":["# Extract the 'label' and 'probability' columns as RDDs\n","predictionAndLabels = prediction_df.select(\"probability\", \"default_payment_next_month\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"default_payment_next_month\"])))\n","\n","\n","# Calculate ROC curve and AUC for each class\n","classes = prediction_df.select(\"default_payment_next_month\").distinct().rdd.flatMap(lambda x: x).collect()\n","roc_auc_scores = {}\n","for class_label in classes:\n","    class_predictions = predictionAndLabels.map(lambda x: (1.0 if x[1] == class_label else 0.0, x[0]))\n","    roc_auc_scores[class_label] = eval.evaluate(class_predictions)\n","\n","# Print the results\n","for class_label, score in roc_auc_scores.items():\n","    print(\"Class:\", class_label)\n","    print(\"ROC AUC Score:\", score)\n","    print()"]},{"cell_type":"code","execution_count":153,"metadata":{"id":"EOtToc1IppMA"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 283.0 failed 1 times, most recent failure: Lost task 0.0 in stage 283.0 (TID 267) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_7644\\2055347043.py\", line 2, in <lambda>\nTypeError: float() argument must be a string or a number, not 'DenseVector'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_7644\\2055347043.py\", line 2, in <lambda>\nTypeError: float() argument must be a string or a number, not 'DenseVector'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[153], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m labels_and_scores \u001b[39m=\u001b[39m prediction_df\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdefault_payment_next_month\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mrdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m row: (\u001b[39mfloat\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mfloat\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mfloat\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mdefault_payment_next_month\u001b[39m\u001b[39m'\u001b[39m])))\n\u001b[0;32m      4\u001b[0m \u001b[39m# Instantiate BinaryClassificationMetrics\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m metrics \u001b[39m=\u001b[39m BinaryClassificationMetrics(labels_and_scores)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:73\u001b[0m, in \u001b[0;36mBinaryClassificationMetrics.__init__\u001b[1;34m(self, scoreAndLabels)\u001b[0m\n\u001b[0;32m     71\u001b[0m sc \u001b[39m=\u001b[39m scoreAndLabels\u001b[39m.\u001b[39mctx\n\u001b[0;32m     72\u001b[0m sql_ctx \u001b[39m=\u001b[39m SQLContext\u001b[39m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m---> 73\u001b[0m numCol \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(scoreAndLabels\u001b[39m.\u001b[39;49mfirst())\n\u001b[0;32m     74\u001b[0m schema \u001b[39m=\u001b[39m StructType(\n\u001b[0;32m     75\u001b[0m     [\n\u001b[0;32m     76\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m     77\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m     78\u001b[0m     ]\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m numCol \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   2889\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   2857\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   2511\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 283.0 failed 1 times, most recent failure: Lost task 0.0 in stage 283.0 (TID 267) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_7644\\2055347043.py\", line 2, in <lambda>\nTypeError: float() argument must be a string or a number, not 'DenseVector'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\rdd.py\", line 2849, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_7644\\2055347043.py\", line 2, in <lambda>\nTypeError: float() argument must be a string or a number, not 'DenseVector'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"]}],"source":["# Extract the 'label' and 'probability' columns as RDDs\n","labels_and_scores = prediction_df.select('prediction', 'probability','default_payment_next_month').rdd.map(lambda row: (float(row['probability']), float(row['prediction']), float(row['default_payment_next_month'])))\n","\n","# Instantiate BinaryClassificationMetrics\n","metrics = BinaryClassificationMetrics(labels_and_scores)"]},{"cell_type":"code","execution_count":186,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7149,"status":"ok","timestamp":1688219651073,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"AClMrgy9tdaM","outputId":"3e962278-9516-4fcf-961b-aa7d8dc26a97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","+--------------------------+----+---+\n","|default_payment_next_month| 0.0|1.0|\n","+--------------------------+----+---+\n","|                         1|1269|683|\n","|                         0|6600|309|\n","+--------------------------+----+---+\n","\n"]}],"source":["\n","# Calculate confusion matrix\n","confusion_matrix = prediction_df.groupBy(\"default_payment_next_month\").pivot(\"prediction\").count().na.fill(0)\n","\n","# # Print the results\n","# print(\"Accuracy:\", accuracy)\n","\n","print(\"Confusion Matrix:\")\n","confusion_matrix.show()"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"1GevWbmDv53h"},"outputs":[],"source":["from pyspark.sql.functions import col\n","\n","# Assuming your label column is called \"label = default_payment_next_month \" converting that column to double\n","prediction_df = prediction_df.withColumn(\"default_payment_next_month\", col(\"default_payment_next_month\").cast(\"double\"))\n"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1688221066553,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"DcDySBS50Ptt","outputId":"acab11d6-ac19-4b2f-8724-b7b4812de04c"},"outputs":[{"data":{"text/plain":["<bound method DataFrame.printSchema of DataFrame[default_payment_next_month: double, scaledFeatures: vector, rawPrediction: vector, probability: vector, prediction: double, probability_double: double]>"]},"execution_count":126,"metadata":{},"output_type":"execute_result"}],"source":["prediction_df.printSchema"]},{"cell_type":"code","execution_count":187,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3795,"status":"ok","timestamp":1688219952102,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"WKfxuDVcu2tR","outputId":"dbc73145-1259-4639-aab9-8c1c9305b542"},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1947.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 412.0 failed 1 times, most recent failure: Lost task 0.0 in stage 412.0 (TID 385) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[187], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m metrics \u001b[39m=\u001b[39m MulticlassMetrics(predictionAndLabels)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Get confusion matrix\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m confusion_matrix \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mconfusionMatrix()\u001b[39m.\u001b[39mtoArray()\n\u001b[0;32m     10\u001b[0m \u001b[39m# Get true positives (TP), true negatives (TN), false positives (FP), false negatives (FN)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tn \u001b[39m=\u001b[39m confusion_matrix[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:311\u001b[0m, in \u001b[0;36mMulticlassMetrics.confusionMatrix\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m1.4.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusionMatrix\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Matrix:\n\u001b[0;32m    307\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m    Returns confusion matrix: predicted classes are in columns,\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    they are ordered by class label ascending, as in \"labels\".\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(\u001b[39m\"\u001b[39;49m\u001b[39mconfusionMatrix\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[1;34m(self, name, *a)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39ma: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m callJavaFunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sc, \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_model, name), \u001b[39m*\u001b[39;49ma)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\mllib\\common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1947.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 412.0 failed 1 times, most recent failure: Lost task 0.0 in stage 412.0 (TID 385) (host.docker.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\BigData\\spark-3.5.0-bin-hadoop3\\spark-3.5.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2160, in verify_struct\n    verifier(v)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2187, in verify\n    verify_value(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2181, in verify_default\n    verify_acceptable_types(obj)\n  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\types.py\", line 2006, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"]}],"source":["# Convert predictions DataFrame to RDD\n","predictionAndLabels = prediction_df.select(\"prediction\", \"default_payment_next_month\").rdd\n","\n","# Instantiate MulticlassMetrics\n","metrics = MulticlassMetrics(predictionAndLabels)\n","\n","# Get confusion matrix\n","confusion_matrix = metrics.confusionMatrix().toArray()\n","\n","# Get true positives (TP), true negatives (TN), false positives (FP), false negatives (FN)\n","tn = confusion_matrix[0][0]\n","fp = confusion_matrix[0][1]\n","fn = confusion_matrix[1][0]\n","tp = confusion_matrix[1][1]\n","\n","# Print the results\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix)\n","\n","print(\"True Negatives (TN):\", tn)\n","print(\"False Positives (FP):\", fp)\n","print(\"False Negatives (FN):\", fn)\n","print(\"True Positives (TP):\", tp)"]},{"cell_type":"code","execution_count":136,"metadata":{"id":"zND-WUpvllTD"},"outputs":[{"ename":"ValueError","evalue":"could not convert string to float: 'default_payment_next_month'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[136], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics\u001b[39m.\u001b[39mprecision(\u001b[39mfloat\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mdefault_payment_next_month\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n","\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'default_payment_next_month'"]}],"source":["metrics.precision(float(\"default_payment_next_month\"))"]},{"cell_type":"markdown","metadata":{"id":"ReZ1VZUOllkQ"},"source":["# Save and Load in Parquet format:"]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.ml.classification.RandomForestClassificationModel"]},"execution_count":193,"metadata":{},"output_type":"execute_result"}],"source":["type(model)"]},{"cell_type":"code","execution_count":216,"metadata":{},"outputs":[{"data":{"text/plain":["'c:\\\\data science\\\\Internship projects\\\\credit card defaulters\\\\Credit_card_default_prediction_with_mlflow\\\\research'"]},"execution_count":216,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","p = os.getcwd()\n","p"]},{"cell_type":"code","execution_count":220,"metadata":{},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o3081.save.\n: java.io.IOException: Path c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\research already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\r\n\tat sun.reflect.GeneratedMethodAccessor321.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[220], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49msave(os\u001b[39m.\u001b[39;49mgetcwd())\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\ml\\util.py:262\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite()\u001b[39m.\u001b[39;49msave(path)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\ml\\util.py:213\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    212\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpath should be a string, got type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(path))\n\u001b[1;32m--> 213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3081.save.\n: java.io.IOException: Path c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\research already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\r\n\tat sun.reflect.GeneratedMethodAccessor321.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"]},{"name":"stderr","output_type":"stream","text":["----------------------------------------\n","Exception occurred during processing of request from ('127.0.0.1', 63816)\n","Traceback (most recent call last):\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n","    self.process_request(request, client_address)\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\socketserver.py\", line 347, in process_request\n","    self.finish_request(request, client_address)\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\socketserver.py\", line 360, in finish_request\n","    self.RequestHandlerClass(request, client_address, self)\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\socketserver.py\", line 720, in __init__\n","    self.handle()\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n","    poll(accum_updates)\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n","    if self.rfile in r and func():\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n","    num_updates = read_int(self.rfile)\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n","    length = stream.read(4)\n","  File \"c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\socket.py\", line 704, in readinto\n","    return self._sock.recv_into(b)\n","ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n","----------------------------------------\n"]}],"source":["model.save()  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oplfi1cellkQ"},"outputs":[],"source":["from pyspark.ml.classification import RandomForestClassificationModel\n","\n","loaded_model = RandomForestClassificationModel.load(\"/content/drive/MyDrive/Machine Learning/credit_card_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1981,"status":"ok","timestamp":1688367290921,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"6gerZIaDllkR","outputId":"09e39380-7cec-480c-e4e1-10f6ddba8f4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+--------------------+--------------------+----------+\n","|default_payment_next_month|      scaledFeatures|       rawPrediction|         probability|prediction|\n","+--------------------------+--------------------+--------------------+--------------------+----------+\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1846322501604...|[0.80923161250802...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.2433734684691...|[0.81216867342345...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1124084631669...|[0.80562042315834...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1846322501604...|[0.80923161250802...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1124084631669...|[0.80562042315834...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1846322501604...|[0.80923161250802...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1124084631669...|[0.80562042315834...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.0536672448581...|[0.80268336224290...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1124084631669...|[0.80562042315834...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1846322501604...|[0.80923161250802...|       0.0|\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1846322501604...|[0.80923161250802...|       0.0|\n","+--------------------------+--------------------+--------------------+--------------------+----------+\n","only showing top 20 rows\n","\n"]}],"source":["predc_df = loaded_model.transform(test_df)\n","predc_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2844,"status":"ok","timestamp":1688367299294,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"6nA_5d90llkR","outputId":"2316e73d-d1d0-4734-fec1-a02c6e132090"},"outputs":[{"data":{"text/plain":["8799"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["predc_df.count()"]},{"cell_type":"markdown","metadata":{"id":"oT6FnbXTllkR"},"source":["## To manually give a sample data from test_df using filter function ,inorder to test with model to test as:\n","##sample input from user"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1893,"status":"ok","timestamp":1688368035128,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"oUSHAMbEllkS","outputId":"c9d4628d-3ee6-4df6-f583-66f60db626a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+\n","|default_payment_next_month|      scaledFeatures|\n","+--------------------------+--------------------+\n","|                         0|(24,[0,1,2,3,4,5,...|\n","+--------------------------+--------------------+\n","\n"]}],"source":["from pyspark.sql.functions import monotonically_increasing_id\n","\n","# Add a unique ID column to the DataFrame\n","test_df_with_id = test_df.withColumn(\"row_id\", monotonically_increasing_id())\n","\n","# Filter out all rows except the third row\n","filtered_df = test_df_with_id.filter(test_df_with_id.row_id == 2)\n","\n","# Remove the row_id column\n","filtered_df = filtered_df.drop(\"row_id\")\n","filtered_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3764,"status":"ok","timestamp":1688368099632,"user":{"displayName":"KRISHNAN UNNI B R","userId":"09723823883460148005"},"user_tz":-330},"id":"yATmoWxXllkS","outputId":"f6946875-938c-40ca-9169-54e96a5575cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------+--------------------+--------------------+--------------------+----------+\n","|default_payment_next_month|      scaledFeatures|       rawPrediction|         probability|prediction|\n","+--------------------------+--------------------+--------------------+--------------------+----------+\n","|                         0|(24,[0,1,2,3,4,5,...|[16.1124084631669...|[0.80562042315834...|       0.0|\n","+--------------------------+--------------------+--------------------+--------------------+----------+\n","\n"]}],"source":["predc_df_user = loaded_model.transform(filtered_df)\n","predc_df_user.show()"]},{"cell_type":"markdown","metadata":{"id":"--8KQ9dvllkS"},"source":["## So we successfully loaded the model and tested it using a test_df"]},{"cell_type":"markdown","metadata":{"id":"P6F5ie7mqAA7"},"source":["\n","# major issue faced was to convert the probability desne vector in prediction datafraem to double format inorder to calculrte roc and auc using \"BinaryClassificationMetrics\""]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["from pyspark import SparkContext\n","sc= spark.sparkContext"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\data science\\Internship projects\\credit card defaulters\\Credit_card_default_prediction_with_mlflow\\venv\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]},{"data":{"text/plain":["0.7083333333333334"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["scoreAndLabels = sc.parallelize([\n","    (0.1, 0.0), (0.1, 1.0), (0.4, 0.0), (0.6, 0.0), (0.6, 1.0), (0.6, 1.0), (0.8, 1.0)], 2)\n","metrics = BinaryClassificationMetrics(scoreAndLabels)\n","metrics.areaUnderROC\n"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"text/plain":["0.8339285714285714"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["metrics.areaUnderPR"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["metrics.unpersist() ## Unpersists intermediate RDDs used in the computation."]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[],"source":["scoreAndLabelsWithOptWeight = sc.parallelize([\n","    (0.1, 0.0, 1.0), (0.1, 1.0, 0.4), (0.4, 0.0, 0.2), (0.6, 0.0, 0.6), (0.6, 1.0, 0.9),\n","    (0.6, 1.0, 0.5), (0.8,  1.0, 0.7)], 2)\n","\n","metrics = BinaryClassificationMetrics(scoreAndLabelsWithOptWeight)\n"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.rdd.RDD"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["type(scoreAndLabelsWithOptWeight)"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[{"data":{"text/plain":["0.791111111111111"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["metrics.areaUnderROC\n"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"data":{"text/plain":["0.8822204401675131"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["\n","metrics.areaUnderPR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWsw7ifhNibcSMOIXHPKuj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
